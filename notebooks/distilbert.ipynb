{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47eb41f",
   "metadata": {},
   "source": [
    "# DistilBERT Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ebcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score,accuracy_score, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59872342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset,concatenate_datasets\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer,DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903f07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873ffbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "val_df   = pd.read_csv(\"../data/raw/val.csv\")\n",
    "test_df  = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdbdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[[\"text\",\"label\"]]\n",
    "val_df   = val_df[[\"text\",\"label\"]]\n",
    "test_df  = test_df[[\"text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8039ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'hatespeech', 1: 'normal', 2: 'offensive'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\"hatespeech\": 0,\"normal\": 1,\"offensive\": 2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fc97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"] = train_df[\"label\"].map(label2id)\n",
    "val_df[\"label\"]   = val_df[\"label\"].map(label2id)\n",
    "test_df[\"label\"]  = test_df[\"label\"].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d7cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223532d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac67f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=\"max_length\",truncation=True,max_length=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b31fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e2408fe4f4bef9c60ef0efe80b899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bc7714f62f4e5bbe4a7c098607f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872c893a418148f9a95a2ec4a0923c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds   = val_ds.map(tokenize, batched=True)\n",
    "test_ds  = test_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb87398",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n",
    "test_ds.set_format(type=\"torch\", columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adba5140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights: tensor([1.0800, 0.8203, 1.1696])\n"
     ]
    }
   ],
   "source": [
    "labels = train_ds[\"label\"]\n",
    "\n",
    "class_counts = torch.bincount(labels)\n",
    "num_classes = len(class_counts)\n",
    "total = labels.size(0)\n",
    "\n",
    "class_weights = total / (num_classes * class_counts.float())\n",
    "print(\"class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26208c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04bb27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels =3,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aef28c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9531f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/distilbert/checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.00001,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "226e344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce(logits, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b556f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = FocalLoss(\n",
    "            alpha=class_weights.to(logits.device),\n",
    "            gamma=2\n",
    "        )\n",
    "\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c5e9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels,preds),\n",
    "        \"macro_f1\": f1_score(labels,preds,average=\"macro\"),\n",
    "        \"weighted_f1\": f1_score(labels,preds,average=\"weighted\"),\n",
    "        \"precision_hate\": precision[0],\n",
    "        \"recall_hate\": recall[0],\n",
    "        \"f1_hate\": f1[0],\n",
    "        \"precision_normal\": precision[1],\n",
    "        \"recall_normal\": recall[1],\n",
    "        \"f1_normal\": f1[1],\n",
    "        \"precision_offensive\": precision[2],\n",
    "        \"recall_offensive\": recall[2],\n",
    "        \"f1_offensive\": f1[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c0fc0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582698d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hario\\AppData\\Local\\Temp\\ipykernel_18684\\2440521205.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = FocalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,      \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "391ab986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1924' max='1924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1924/1924 10:31, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Precision Hate</th>\n",
       "      <th>Recall Hate</th>\n",
       "      <th>F1 Hate</th>\n",
       "      <th>Precision Normal</th>\n",
       "      <th>Recall Normal</th>\n",
       "      <th>F1 Normal</th>\n",
       "      <th>Precision Offensive</th>\n",
       "      <th>Recall Offensive</th>\n",
       "      <th>F1 Offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>0.317442</td>\n",
       "      <td>0.629553</td>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.627596</td>\n",
       "      <td>0.728083</td>\n",
       "      <td>0.826307</td>\n",
       "      <td>0.774092</td>\n",
       "      <td>0.799544</td>\n",
       "      <td>0.449424</td>\n",
       "      <td>0.575410</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.673358</td>\n",
       "      <td>0.543446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.306018</td>\n",
       "      <td>0.676379</td>\n",
       "      <td>0.678361</td>\n",
       "      <td>0.681702</td>\n",
       "      <td>0.789855</td>\n",
       "      <td>0.735245</td>\n",
       "      <td>0.761572</td>\n",
       "      <td>0.754947</td>\n",
       "      <td>0.635083</td>\n",
       "      <td>0.689847</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.583664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.304042</td>\n",
       "      <td>0.687305</td>\n",
       "      <td>0.686945</td>\n",
       "      <td>0.690858</td>\n",
       "      <td>0.766831</td>\n",
       "      <td>0.787521</td>\n",
       "      <td>0.777038</td>\n",
       "      <td>0.778471</td>\n",
       "      <td>0.638924</td>\n",
       "      <td>0.701828</td>\n",
       "      <td>0.528274</td>\n",
       "      <td>0.647810</td>\n",
       "      <td>0.581967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.303823</td>\n",
       "      <td>0.680021</td>\n",
       "      <td>0.678551</td>\n",
       "      <td>0.682222</td>\n",
       "      <td>0.741538</td>\n",
       "      <td>0.812816</td>\n",
       "      <td>0.775543</td>\n",
       "      <td>0.784666</td>\n",
       "      <td>0.615877</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.522003</td>\n",
       "      <td>0.627737</td>\n",
       "      <td>0.570008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1924, training_loss=0.29452721096126055, metrics={'train_runtime': 632.5123, 'train_samples_per_second': 97.282, 'train_steps_per_second': 3.042, 'total_flos': 2037782333832192.0, 'train_loss': 0.29452721096126055, 'epoch': 4.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0457d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(val_ds)\n",
    "\n",
    "logits = preds.predictions\n",
    "y_true = preds.label_ids\n",
    "y_pred = np.argmax(logits,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ecebc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 1922)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_idx = np.where(y_pred != y_true)[0]\n",
    "len(wrong_idx),len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99d92733",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_val_ds = val_ds.select(wrong_idx)\n",
    "hard_boost = concatenate_datasets([hard_val_ds,hard_val_ds])\n",
    "boosted_train_ds = concatenate_datasets([train_ds, hard_boost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26edd8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels =3,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f86a7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/distilbert/checkpoints2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.000008,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0243980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hario\\AppData\\Local\\Temp\\ipykernel_18684\\3156783260.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = FocalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=boosted_train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44cf42a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2076/2076 11:14, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Precision Hate</th>\n",
       "      <th>Recall Hate</th>\n",
       "      <th>F1 Hate</th>\n",
       "      <th>Precision Normal</th>\n",
       "      <th>Recall Normal</th>\n",
       "      <th>F1 Normal</th>\n",
       "      <th>Precision Offensive</th>\n",
       "      <th>Recall Offensive</th>\n",
       "      <th>F1 Offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.296263</td>\n",
       "      <td>0.656608</td>\n",
       "      <td>0.661821</td>\n",
       "      <td>0.661570</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.774030</td>\n",
       "      <td>0.779287</td>\n",
       "      <td>0.792381</td>\n",
       "      <td>0.532650</td>\n",
       "      <td>0.637060</td>\n",
       "      <td>0.476601</td>\n",
       "      <td>0.706204</td>\n",
       "      <td>0.569118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.259265</td>\n",
       "      <td>0.687305</td>\n",
       "      <td>0.689332</td>\n",
       "      <td>0.690370</td>\n",
       "      <td>0.782677</td>\n",
       "      <td>0.838111</td>\n",
       "      <td>0.809446</td>\n",
       "      <td>0.819013</td>\n",
       "      <td>0.573624</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.508108</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.583851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.231605</td>\n",
       "      <td>0.737253</td>\n",
       "      <td>0.730604</td>\n",
       "      <td>0.736025</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.881956</td>\n",
       "      <td>0.834796</td>\n",
       "      <td>0.792958</td>\n",
       "      <td>0.720871</td>\n",
       "      <td>0.755198</td>\n",
       "      <td>0.599638</td>\n",
       "      <td>0.604015</td>\n",
       "      <td>0.601818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.723725</td>\n",
       "      <td>0.724760</td>\n",
       "      <td>0.726618</td>\n",
       "      <td>0.815873</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.840556</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.629962</td>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.548433</td>\n",
       "      <td>0.702555</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2076, training_loss=0.31819225873561263, metrics={'train_runtime': 674.3596, 'train_samples_per_second': 98.375, 'train_steps_per_second': 3.078, 'total_flos': 2197010986583040.0, 'train_loss': 0.31819225873561263, 'epoch': 4.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9520a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/distilbert/final_model\\\\tokenizer_config.json',\n",
       " '../models/distilbert/final_model\\\\special_tokens_map.json',\n",
       " '../models/distilbert/final_model\\\\vocab.txt',\n",
       " '../models/distilbert/final_model\\\\added_tokens.json',\n",
       " '../models/distilbert/final_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../models/distilbert/final_model\")\n",
    "tokenizer.save_pretrained(\"../models/distilbert/final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
